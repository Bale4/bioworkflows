{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Identifying loci of interest from association study results\n",
    "\n",
    "The aim of this workflow is to select subsets of SNPs of potential interest (low p-value) from GWAS for follow up analysis (such as fine-mapping). We use the LD clumping method to account for LD between SNPs. The outcome of LD clumping is sets of SNPs in LD with each other representing association signals for each locus of interest. SNPs between different clumps are independent. SNPs in a clump can be used to establish boundaries for loci to be investigated further.\n",
    "\n",
    "LD clumping is implemented in PLINK. A practical challenge is that some LD reference genotype data, such as that from UK Biobank, is too large to efficiently use for LD clumping. We implemented this pipeline to allow for determining LD based only on selected samples. We have successfully applied the procedure in the UK Biobank data we have analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "**A note to readers:** please read through the \"Input\", \"Parameter setting\", \"Output\" sections, then follow the instructions in \"Minimal working example illustration\" to complete an exercise analysis with the minimal working example data we provide. The code implementations of the analysis is available as the rest of the notebook; interested readers may read the section \"Command Interface\" and onwards to learn about the implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input\n",
    "\n",
    "- GWAS summary statistics\n",
    "- LD reference panel\n",
    "\n",
    "## Parameter settings for clumping\n",
    "\n",
    "1. Which reference dataset to use? \n",
    "\n",
    "    * Public data such as 1000 Genomes Project (eg our `1000G_CEU`  bundle), HapMap (eg our `hapmap_CEU_r23a_filtered` bundle), UK10K, HRC reference panel\n",
    "    * In-sample LD: use the same genotype files that the GWAS data were generated, if available\n",
    "    \n",
    "2. What is the significance threshold for the index variant (p1) we should use for the analyses? \n",
    "    \n",
    "    p=5e-08\n",
    "    \n",
    "3. What significance threshold to use for the SNPs to be clumped? \n",
    "   \n",
    "   p=1 (this will include all the SNPs in LD with the index SNP)\n",
    "   \n",
    "4. What LD $r^2$ to use? \n",
    "   \n",
    "   r2=0.3 or even lower to capture bigger LD blocks\n",
    "   \n",
    "5. What window size in kb to use? \n",
    "   \n",
    "   We use 2Mb in this analysis which is way larger than the average LD block size in humans\n",
    "   \n",
    "Below are the default options used by PLINK\n",
    "\n",
    "```\n",
    "--clump-p1 0.0001: significance threshold for Index SNPs\n",
    "--clump-p2 0.01: Secondary significance threshold for clumped SNPs\n",
    "--clump-r2 0.50: LD threshold for clumping\n",
    "--clump-kb 250: Physical distance threshold for clumping\n",
    "--clump-field P_value: To specify the name of the field for P-value\n",
    "--clump-verbose: to add a more detailed report of SNPs in each clump\n",
    "--clump-best: to select the single best proxy\n",
    "--clump-allow-overlap: allow for overlap between clumped regions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output\n",
    "\n",
    "A list of regions in the following format:\n",
    "\n",
    "```\n",
    "chr start end top_snp all_snps\n",
    "```\n",
    "\n",
    "where the last column `all_snps` are SNP IDs in the LD cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal working example illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "We demonstrate this pipeline on a minimal working example (MWE) dataset generated by the `LMM.ipynb` workflow, and is available on [Google Drive](https://drive.google.com/drive/u/0/folders/1h931uJPKuCQyh_vi08Xfjn2NnzI2AZmt) (access to be granted upon request). The data-set, being a toy example, does not have a strong GWAS signal. We will illustrate clumping for association tests with p-value < 1E-5. The LD will be based on 200 samples as an illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Running \u001b[32mdefault\u001b[0m: Perform LD-clumping in PLINKv1.9\n",
      "INFO: Running \u001b[32mreference_1\u001b[0m: \n",
      "INFO: Running \u001b[32mfilter_plink\u001b[0m: Select a subset of samples from the plink files\n",
      "INFO: \u001b[32mfilter_plink\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mfilter_plink\u001b[0m output:   \u001b[32mclumping_output/cache/1000G.EUR.mwe.pruned.200.bed\u001b[0m\n",
      "INFO: \u001b[32mreference_1\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mreference_1\u001b[0m output:   \u001b[32mclumping_output/cache/1000G.EUR.mwe.pruned.200.bed\u001b[0m\n",
      "INFO: Running \u001b[32mreference_2\u001b[0m: Merge all the .bed files into one reference file to use in clumping step\n",
      "INFO: \u001b[32mreference_2\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mreference_2\u001b[0m output:   \u001b[32mclumping_output/mwe.200.ref_geno.bed\u001b[0m\n",
      "INFO: \u001b[32mdefault\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mdefault\u001b[0m output:   \u001b[32mclumping_output/1000G.EUR.pheno_x.regenie.snp_stats.clumped clumping_output/1000G.EUR.pheno_x.regenie.snp_stats.clumped_region\u001b[0m\n",
      "INFO: Workflow default (ID=wc1f850dcba7e55a1) is executed successfully with 4 completed steps.\n"
     ]
    }
   ],
   "source": [
    "sos run LD_Clumping.ipynb \\\n",
    "    --cwd clumping_output \\\n",
    "    --genoFile data/1000G.EUR.mwe.pruned.bed \\\n",
    "    --sampleFile data/1000G.EUR.mwe.pruned.fam \\\n",
    "    --sumstatsFiles data/1000G.EUR.pheno_x.regenie.snp_stats.gz \\\n",
    "    --clump-p1 0.0005 \\\n",
    "    --clump-field P \\\n",
    "    --clump-annotate BP \\\n",
    "    --ld-sample-size 200 \\\n",
    "    --reference-genotype-prefix mwe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "The clumped regions can be found as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 248715699 249172500\n",
      "2 152287831 152331133\n",
      "2 211348599 211499959\n",
      "4 11788538 11807469\n",
      "4 132424256 132586858\n",
      "4 166669197 167485564\n",
      "5 32500390 32625480\n",
      "5 100665906 100883179\n",
      "5 145830216 145926587\n",
      "6 4526996 4551432\n"
     ]
    }
   ],
   "source": [
    "head clumping_output/1000G.EUR.pheno_x.regenie.snp_stats.clumped_region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "These regions can be used in subsequent workflows for refined analysis including statistical fine-mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run LD_Clumping.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Global parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Working directory: change accordingly\n",
    "parameter: cwd = path\n",
    "# Path to bgen or plink files\n",
    "parameter: genoFile = paths\n",
    "# Path to sample files\n",
    "parameter: sampleFile = path\n",
    "# Path to summary stats file\n",
    "parameter: sumstatsFiles = paths\n",
    "# Path to samples of unrelated individuals\n",
    "parameter: unrelated_samples = path(\".\")\n",
    "# Reference genotype file\n",
    "parameter: reference_genotype_prefix = str\n",
    "# Number of samples to use to compute LD\n",
    "parameter: ld_sample_size = 2000\n",
    "# Clumping parameteres\n",
    "parameter: clump_field = str\n",
    "parameter: clump_annotate = \"\"\n",
    "parameter: clump_p1 = 5e-08\n",
    "parameter: clump_p2 = 0.01\n",
    "# r2 = 0.04 => r = 0.2\n",
    "parameter: clump_r2 = 0.04\n",
    "parameter: clump_kb = 2000\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# File names for clumping\n",
    "parameter: clumpFile = path(f'{cwd}/' + \"_\".join([f'{x:bn}' for x in sumstatsFiles]) + '.clumped')\n",
    "# Output the bgen file with 8bit formatting\n",
    "#parameter: bgen_bits=16\n",
    "# Specific number of threads to use\n",
    "parameter: numThreads = 5\n",
    "# Specify the container to use\n",
    "parameter: container = ''\n",
    "if not container:\n",
    "    container = None\n",
    "if unrelated_samples == path(\".\"):\n",
    "    unrelated_samples = sampleFile\n",
    "clumpregionFile = f'{clumpFile:n}.clumped_region'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Select a random subset of unrelated samples\n",
    "\n",
    "The unrelated sample file should be a text file containing white space separated columns. It should have a column named `IID` for sample IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Create a white-space delimited file with a list of unrelated samples in data.\n",
    "[filter_samples: provides = f'{cwd}/{unrelated_samples:bn}.{ld_sample_size}.txt']\n",
    "input: unrelated_samples, sampleFile\n",
    "output: f'{cwd}/{unrelated_samples:bn}.{ld_sample_size}.txt'\n",
    "R: container = container, expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    set.seed(1)\n",
    "    all_unrelated = read.table(${_input[0]:r}, header=F${\"\" if _input[0].suffix == \".fam\" else \", skip=2\"})\n",
    "    avail_samples = read.table(${_input[1]:r}, header=F${\"\" if _input[1].suffix == \".fam\" else \", skip=2\"})\n",
    "    unrelated_samples = which(avail_samples[,1] %in% all_unrelated[,1])\n",
    "    dat = avail_samples[unrelated_samples,]\n",
    "    if (${ld_sample_size} < nrow(dat)) {\n",
    "      dat = dat[sample(1:nrow(dat), ${ld_sample_size}), 1, drop=F]\n",
    "    } else {\n",
    "      dat = dat[, 1, drop=F]\n",
    "    }\n",
    "    write.table(dat, ${_output:r}, quote=F, row.names=F, col.names=F)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Select Filter the BGEN files \n",
    "\n",
    "This step is based on the samples selected in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Select a subset of samples from the bgen files\n",
    "[filter_bgen_1]\n",
    "depends: f'{cwd}/{unrelated_samples:bn}.{ld_sample_size}.txt'\n",
    "input: genoFile, group_by=1\n",
    "output: f'{cwd}/cache/{_input:bn}.{ld_sample_size}.bgen'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '48h',  mem = '60G', tags = f'{step_name}_{_output:bn}'\n",
    "bash: container = container, expand= True, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "   qctool -g {_input} -s {sampleFile} -og {_output} -os {_output:n}.sample -incl-samples {_depends}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Make PLINK files with the selected samples\n",
    "\n",
    "The BGEN files extracted have to be converted to PLINK format because currently LD clumping in PLINK 1.9 does not work with BGEN format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Make the binary files for bgen input using the selected samples and exclude repeated variant ids\n",
    "[filter_bgen_2]\n",
    "depends: Py_Module('xxhash')\n",
    "output: f'{cwd}/cache/{_input:bn}.bed'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '48h', mem = '60G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container = container, expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    # try create index if not exist\n",
    "    bgenix -g ${_input} -index || true\n",
    "    # get a list of duplicated SNP IDs\n",
    "    bgenix -g ${_input} -list 2>/dev/null | grep -v \"#\" | tail -n+2 | cut -d$'\\t' -f2 | sort | uniq -d > ${_output:n}.exclude\n",
    "    plink2 \\\n",
    "    --bgen ${_input} ref-first \\\n",
    "    --sample ${_input:n}.sample \\\n",
    "    --make-bed \\\n",
    "    --exclude ${_output:n}.exclude \\\n",
    "    --out ${_output:n} \\\n",
    "    --threads ${numThreads} \\\n",
    "    --memory 12000\n",
    "    \n",
    "python: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    # Fix SNP names longer than 50 characters. \n",
    "    # This will result in a false insufficient memory alert and error in the next step, if not dealt with\n",
    "    import pandas as pd\n",
    "    from xxhash import xxh32 as xxh\n",
    "    def shorten_id(x):\n",
    "        return x if len(x) < 30 else f\"{x.split('_')[0]}_{xxh(x).hexdigest()}\"\n",
    "\n",
    "    dat = pd.read_csv('${_output:n}.bim', header=None, sep='\\t')\n",
    "    dat.columns = ['chrom', 'id', 'gd', 'pos', 'a1', 'a2']\n",
    "    dat['id'] = dat['id'].apply(shorten_id)\n",
    "    dat.to_csv('${_output:n}.bim', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Filter PLINK files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Select a subset of samples from the plink files\n",
    "[filter_plink]\n",
    "depends: f'{cwd}/{unrelated_samples:bn}.{ld_sample_size}.txt'\n",
    "input: genoFile, group_by=1\n",
    "output: f'{cwd}/cache/{_input:bn}.{ld_sample_size}.bed'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '48h',  mem = '60G', tags = f'{step_name}_{_output:bn}'\n",
    "bash: container = container, expand= True, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    plink --bfile {_input:n} --keep-fam {_depends} --make-bed --out {_output:n} --threads {numThreads} --memory 12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[reference_1]\n",
    "bgen = [x for x in genoFile if x.suffix == '.bgen']\n",
    "plink = [x for x in genoFile if x.suffix == '.bed']\n",
    "input: genoFile, group_by=1\n",
    "output: f'{cwd}/cache/{_input:bn}.{ld_sample_size}.bed'\n",
    "if len(bgen):\n",
    "    sos_run('filter_bgen', genoFile=bgen)\n",
    "if len(plink):\n",
    "    sos_run('filter_plink', genoFile=plink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Merge all chroms to one file\n",
    "\n",
    "This is necessary for LD clumping in PLINK to work properly. We cannot `--merge` and `--make-bed` starting from bgen files in PLINK 2 at this point. We have to stick to PLINK 1.9 which requires duplicated variants have to be removed and indels renamed to <50 characters in length, because PLINK1.9 is not capable of dealing with very long variant names and when merging different bed files it cannot handle multiallelic variants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Merge all the .bed files into one reference file to use in clumping step\n",
    "[reference_2]\n",
    "input: group_by='all'\n",
    "output: f'{cwd}/{reference_genotype_prefix}.{ld_sample_size}.ref_geno.bed'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '48h', mem = '60G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container = container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    echo -e ${' '.join([str(x)[:-4] for x in _input[1:]])} | sed 's/ /\\n/g' > ${_output:n}.merge_list\n",
    "    plink \\\n",
    "    --bfile ${_input[0]:n} \\\n",
    "    --merge-list ${_output:n}.merge_list \\\n",
    "    --make-bed \\\n",
    "    --out ${_output:n} \\\n",
    "    --threads ${numThreads} \\\n",
    "    --memory 48000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Perform LD clumping per chrom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Note: The same fields are extracted from all results files (e.g. SNP and P) -- i.e. it is not possible to specify different fields from different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Perform LD-clumping in PLINKv1.9\n",
    "[default]\n",
    "parameter: verbose = True\n",
    "input: f'{cwd}/{reference_genotype_prefix}.{ld_sample_size}.ref_geno.bed'\n",
    "output: clumpFile, clumpregionFile \n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '48h', mem = '48G',cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: container = container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'   \n",
    "    plink \\\n",
    "    --bfile ${_input:n} \\\n",
    "    --clump ${sumstatsFiles:,} \\\n",
    "    --clump-field ${clump_field} \\\n",
    "    --clump-p1 ${clump_p1} \\\n",
    "    --clump-p2 ${clump_p2} \\\n",
    "    --clump-r2 ${clump_r2} \\\n",
    "    --clump-kb ${clump_kb} \\\n",
    "    ${(\"--clump-verbose\") if verbose else \"\"} \\\n",
    "    ${(\"--clump-annotate %s\" % clump_annotate) if clump_annotate else \"\"} \\\n",
    "    --clump-allow-overlap \\\n",
    "    --out ${_output[0]:n} \\\n",
    "    --threads ${numThreads} \\\n",
    "    && touch ${_output[0]} # need to touch and create empty file because some chroms may not have anything significant to clump.\n",
    "    grep \"RANGE\" ${_output[0]} | awk -F \":\" '{print $2, $3}' | sort -V | sed 's/\\../ /g; s/^[[:blank:]]*//g ; s/chr//g' > ${_output[1]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.4"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
