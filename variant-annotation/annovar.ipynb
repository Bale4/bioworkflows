{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Annotation of exome variants using Annovar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Aim\n",
    "\n",
    "Prepare the data for further association analyses using the LMM.ipynb on rare variants. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Description of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This pipeline provides 3 different possibilities depending on the type of input data you are starting with:\n",
    "\n",
    "### Scenario 1 : you have multiple bim files (e.g. one per chromosome) and you want to merge them into one file for later annotation with annovar\n",
    "\n",
    "Run `bim_merge` to concatenate all the bim files and then run `annovar` to annotate all the variants at once\n",
    "\n",
    "### Scenario 2: you either want to work with common or rare variants.\n",
    "\n",
    "Run `get_snps` using the `--maf` or `max-maf` depending on the type of variants you would like to extract and then run `annovar`\n",
    "\n",
    "### Scenario 3: you already have a specific list of variants you would like to annotate stored in a bim file. \n",
    "\n",
    "Run `annovar`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to start kernel \"Bash\". No such kernel named calysto_bash\nError Message:\n",
     "execution_count": 1,
     "output_type": "error",
     "status": "error",
     "traceback": [
      "\u001b[91mFailed to start kernel \"Bash\". No such kernel named calysto_bash\nError Message:\n\u001b[0m"
     ]
    }
   ],
   "source": [
    "sos run annovar.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Illustration with minimal working example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "**Scenario 3:** On Yale's cluster, here modify humandb and ukbb paths to match the location of the databases needed by annovar to function\n",
    "\n",
    "```\n",
    "sos run ~/project/bioworkflows/variant-annotation/annovar.ipynb annovar \\\n",
    "    --cwd output \\\n",
    "    --bim_name ukb23156_c22.merged.filtered.bim \\\n",
    "    --humandb /gpfs/ysm/datasets/db/annovar/humandb \\\n",
    "    --ukbb /gpfs/gibbs/pi/dewan/data/UKBiobank \\\n",
    "    --job_size 1 \\\n",
    "    --name_prefix mwe_chr22 \\\n",
    "    --container_annovar /gpfs/gibbs/pi/dewan/data/UKBiobank/annovar.sif\n",
    "```\n",
    "\n",
    "On Columbia's cluster\n",
    "\n",
    "```\n",
    "sos run ~/project/bioworkflows/variant-annotation/annovar.ipynb annovar \\\n",
    "    --cwd output \\\n",
    "    --bim_name /mnt/mfs/statgen/UKBiobank/data/exome_files/project_VCF/plink_files/ukb23156_c22.merged.filtered.bim \\\n",
    "    --humandb /mnt/mfs/statgen/isabelle/REF/humandb  \\\n",
    "    --ukbb /mnt/mfs/statgen/isabelle/REF/humandb \\\n",
    "    --job_size 1 \\\n",
    "    --name_prefix mwe_chr22 \\\n",
    "    --container_annovar /mnt/mfs/statgen/containers/gatk4-annovar.sif\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# the output directory for generated files\n",
    "parameter: cwd = path\n",
    "# Specific number of threads to use\n",
    "parameter: numThreads = 2\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Human genome build\n",
    "parameter: build = 'hg38'\n",
    "# Name for the merged bimfiles\n",
    "parameter: bim_name = path\n",
    "# Prefix for the name based on common/rare variant filtering\n",
    "parameter: name_prefix = str\n",
    "# Load annovar module from cluster\n",
    "parameter: annovar_module = '''\n",
    "module load ANNOVAR/2020Jun08-foss-2018b-Perl-5.28.0\n",
    "echo \"Module annovar loaded\"\n",
    "{cmd}\n",
    "'''\n",
    "# Software container option\n",
    "parameter: container_annovar = 'gaow/gatk4-annovar'\n",
    "parameter: container_lmm = 'statisticalgenetics/lmm:2.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Format file for plink .bim\n",
    "\n",
    "A text file with no header line, and one line per variant with the following six fields:\n",
    "1. Chromosome code (either an integer, or 'X'/'Y'/'XY'/'MT'; '0' indicates unknown) or name\n",
    "2. Variant identifier\n",
    "3. Position in morgans or centimorgans (safe to use dummy value of '0')\n",
    "4. Base-pair coordinate (1-based; limited to 231-2)\n",
    "5. Allele 1 (corresponding to clear bits in .bed; usually minor)\n",
    "6. Allele 2 (corresponding to set bits in .bed; usually major)\n",
    "\n",
    "In the bim file the second column e.g `1:930232:C:T` contains the alleles in ref/alt mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Merge all the bimfiles into a single file to use later with awk\n",
    "# Only need to run this cell once\n",
    "[bim_merge: provides = bim_name]\n",
    "parameter: bimfiles = paths\n",
    "input: bimfiles\n",
    "output: bim_name\n",
    "task: trunk_workers = 1, walltime = '10h', mem = '10G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout' \n",
    "      cat ${_input} > ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Get a list of common SNPs above (--maf) or rare SNPs below (--max-maf) certain MAF\n",
    "[get_snps_1]\n",
    "# bed files plink format\n",
    "parameter: bfiles = paths\n",
    "# Filter based on minor allele frequency (use when filtering common variants)\n",
    "parameter: maf_filter = 0.0\n",
    "# Filter based on the maximum maf allowed (use when filtering rare variants)\n",
    "parameter: max_maf_filter = 0.001\n",
    "# Filter out variants with missing call rate higher that this value\n",
    "parameter: geno_filter = 0.0\n",
    "# Filter according to Hardy Weiberg Equilibrium\n",
    "parameter: hwe_filter = 0.0\n",
    "# Fitler out samples with missing rate higher than this value\n",
    "parameter: mind_filter = 0.0\n",
    "input: bfiles, group_by=1\n",
    "output: f'{cwd}/cache/{_input:bn}.{name_prefix}.snplist'\n",
    "task: trunk_workers = 1, walltime = '10h', mem = '30G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container=container_lmm, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout' \n",
    "    plink2 \\\n",
    "      --bfile ${_input:n}\\\n",
    "      ${('--maf %s' % maf_filter) if maf_filter > 0 else ''} \\\n",
    "      ${('--max-maf %s' % max_maf_filter) if max_maf_filter > 0 else ''} \\\n",
    "      ${('--geno %s' % geno_filter) if geno_filter > 0 else ''} \\\n",
    "      ${('--hwe %s' % hwe_filter) if hwe_filter > 0 else ''} \\\n",
    "      ${('--mind %s' % mind_filter) if mind_filter > 0 else ''} \\\n",
    "      --write-snplist --no-id-header\\\n",
    "      --freq \\\n",
    "      --threads ${numThreads} \\\n",
    "      --out ${_output:n} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Merge all of the common_var.snplist into a single file and all the rare_var.snplist into another single file\n",
    "[get_snps_2]\n",
    "input: group_by='all'\n",
    "output: f'{cwd}/cache/{name_prefix}.snplist'\n",
    "task: trunk_workers = 1, walltime = '10h', mem = '10G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output:n}.stdout' \n",
    "      cat ${_input} > ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Search for common or rare variants in bimfile and generate annovar input file\n",
    "[get_snps_3]\n",
    "depends: bim_name\n",
    "output: f'{cwd}/{_input:bn}.avinput'\n",
    "task: trunk_workers = 1, walltime = '10h', mem = '10G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout' \n",
    "    awk -F\" \" 'FNR==NR {lines[$1]; next} $2 in lines ' ${_input} ${bim_name} > ${_output:n}.tmp\n",
    "    awk '{if ($2 ~ /D/) {print $1, $4, $4 + (length ($6) - length ($5)), $6, $5 } else {print $1, $4, $4, $6, $5 }}'  ${_output:n}.tmp >  ${_output}\n",
    "    # remove temporary files\n",
    "    rm -f ${_output:n}.tmp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Annovar details\n",
    "\n",
    "For a list of available [databases](https://hgdownload.soe.ucsc.edu/goldenPath/hg38/database/)\n",
    "\n",
    "On Farnam's Yale HPC there is a folder for shared databases\n",
    "```/gpfs/ysm/datasets/db/annovar/humandb``` \n",
    "\n",
    "and a folder for the x_ref database ```/gpfs/gibbs/pi/dewan/data/UKBiobank/mart_export_2019_LOFtools3.txt```\n",
    "\n",
    "On Yale's cluster there folder for shared databases for build hg19 is under Isabelle's folder\n",
    "```/mnt/mfs/statgen/isabelle/REF/humandb```\n",
    "\n",
    "and the x_ref database is under that same folder ```/mnt/mfs/statgen/isabelle/REF/humandb```\n",
    "\n",
    "\n",
    "### Important note\n",
    "\n",
    "Please make sure you are using the correct build for your annotations UKBB exome data for 200K individuals need hg38 build\n",
    "\n",
    "### Format file for annovar input\n",
    "\n",
    "On each line, the first five space- or tab- delimited columns represent \n",
    "\n",
    "1. chromosome \n",
    "2. start position \n",
    "3. end position \n",
    "4. the reference nucleotides\n",
    "5. the observed nucleotides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Create annovar input file\n",
    "[annovar_1]\n",
    "input: bim_name\n",
    "output: f'{cwd}/{_input:bn}.{build}.avinput'\n",
    "task: trunk_workers = 1, walltime = '10h', mem = '10G', cores = numThreads, tags = f'{_output:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.err', stdout = f'{_output:n}.out' \n",
    "    awk '{if ($2 ~ /D/) {print $1, $4, $4 + (length ($6) - length ($5)), $6, $5, $2} else {print $1, $4, $4, $6, $5, $2}}'  ${_input} >  ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Annotate vcf file using ANNOVAR\n",
    "[annovar_2]\n",
    "# humandb path for ANNOVAR\n",
    "parameter: humandb = path\n",
    "parameter: ukbb = path\n",
    "#add xreffile to option without -exonicsplicing\n",
    "#mart_export_2019_LOFtools3.txt #xreffile latest option -> Phenotype description,HGNC symbol,MIM morbid description,CGD_CONDITION,CGD_inh,CGD_man,CGD_comm,LOF_tools\n",
    "parameter: x_ref = path(f\"{ukbb}/mart_export_2019_LOFtools3.txt\")\n",
    "# Annovar protocol\n",
    "parameter: protocol = ['refGene', 'refGeneWithVer', 'knownGene', 'ensGene', 'phastConsElements30way', 'encRegTfbsClustered', 'gwasCatalog', 'gnomad211_genome', 'gnomad211_exome', 'gme', 'kaviar_20150923', 'abraom', 'avsnp150', 'dbnsfp41a', 'dbscsnv11', 'regsnpintron', 'clinvar_20200316', 'gene4denovo201907']\n",
    "# Annovar operation\n",
    "parameter: operation = ['g', 'g', 'g', 'gx', 'r', 'r', 'r', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f']\n",
    "# Annovar args\n",
    "parameter: arg = ['\"-splicing 12 -exonicsplicing\"', '\"-splicing 30\"', '\"-splicing 12 -exonicsplicing\"', '\"-splicing 12\"', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
    "output: f'{cwd}/{_input:bn}.{build}_multianno.csv'\n",
    "task: trunk_workers = 1, walltime = '60h', mem = '48G', cores = numThreads, tags = f'{_output:bn}'\n",
    "bash: container=container_annovar, volumes=[f'{humandb:a}:{humandb:a}', f'{x_ref:ad}:{x_ref:ad}'], expand=\"${ }\", stderr=f'{_output:n}.err', stdout=f'{_output:n}.out'\n",
    "    #do not add -intronhgvs as option -> writes cDNA variants as HGVS but creates issues (+2 splice site reported only)\n",
    "    #-nastring . can only be . for VCF files\n",
    "    #regsnpintron might cause shifted lines (be carefull using)\n",
    "    table_annovar.pl \\\n",
    "        ${_input} \\\n",
    "        ${humandb} \\\n",
    "        -buildver ${build} \\\n",
    "        -out ${_output:nn}\\\n",
    "        -remove \\\n",
    "        -polish \\\n",
    "        -nastring . \\\n",
    "        -protocol ${\",\".join(protocol)} \\\n",
    "        -operation ${\",\".join(operation)} \\\n",
    "        -arg ${\",\".join(arg)} \\\n",
    "        -csvout \\\n",
    "        -xreffile ${x_ref}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "calysto_bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
