{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## DeepSea Pipeline:\n",
    "\n",
    "This is the code to run the minimal working example for LDSC with DeepSea Integration. The code will train the deepsea model on the set of features provided using the .yml file on the google drive folder, get predictions on the reference genome from the trained model, and run LDSC on the resulting predictions to output enrichments. \n",
    "\n",
    "If you would like to use a different set of features or change training parameters, please edit the .yml file provided and everything else will still work.\n",
    "\n",
    "## Command Interface:\n",
    "\n",
    "This is the list of commands and workflows with explanations for each one, detailed information for each step will be presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No help information is available for script run: Failed to locate LDSC_DeepSea_Code.ipynb.sos\r\n"
     ]
    }
   ],
   "source": [
    "!sos run LDSC_DeepSea_Code.ipynb -h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python 3 (ipykernel)"
   },
   "source": [
    "## Background:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Training Model: \n",
    "\n",
    "**Workflow Command to train model for Minimal Example:** `sos run LDSC_DeepSea_Code.ipynb train_model`\n",
    "\n",
    "To train the model using different data you can use the .yml file provided to change the training parameters and files. An example of how this file looks for the minimal working example is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "ops: [train, evaluate]\n",
      "model: {\n",
      "    path: /mnt/mfs/statgen/Anmol/training_files/deeperdeepsea.py,#UPDATE\n",
      "    class: DeeperDeepSEA,\n",
      "    class_args: {\n",
      "        sequence_length: 1000,\n",
      "        n_targets: 7,\n",
      "    },\n",
      "    non_strand_specific: mean\n",
      "}\n",
      "sampler: !obj:selene_sdk.samplers.IntervalsSampler {\n",
      "    reference_sequence: !obj:selene_sdk.sequences.Genome {\n",
      "        input_path: /mnt/mfs/statgen/Anmol/training_files/male.hg19.fasta,#UPDATE\n",
      "        blacklist_regions: hg19\n",
      "    },\n",
      "    features: !obj:selene_sdk.utils.load_features_list {\n",
      "        input_path: /mnt/mfs/statgen/Anmol/training_files/tutorial/tutorial_features.txt #UPDATE\n",
      "    },\n",
      "    target_path: /mnt/mfs/statgen/Anmol/training_files/tutorial/tutorial.bed.gz, #UPDATE\n",
      "    intervals_path: /mnt/mfs/statgen/Anmol/training_files/DNase_Intervals_FULL.txt, #UPDATE\n",
      "    seed: 127,\n",
      "    # A positive example is an 1000bp sequence with at least 1 class/feature annotated to it.\n",
      "    # A negative sample has no classes/features annotated to the sequence.\n",
      "    sample_negative: True,\n",
      "    sequence_length: 1000,\n",
      "    center_bin_to_predict: 200,\n",
      "    test_holdout: 0.2,\n",
      "    validation_holdout: 0.3,\n",
      "    # The feature must take up 50% of the bin (200bp) for it to be considered\n",
      "    # a feature annotated to that sequence.\n",
      "    feature_thresholds: 0.25,\n",
      "    mode: train,\n",
      "    save_datasets: [validate, test]\n",
      "}\n",
      "train_model: !obj:selene_sdk.TrainModel {\n",
      "    batch_size: 64,\n",
      "    max_steps: 501,  # update this value for longer training\n",
      "    report_stats_every_n_steps: 250,\n",
      "    n_validation_samples: 6000,\n",
      "    n_test_samples: 22000,\n",
      "    cpu_n_threads: 32,\n",
      "    use_cuda: False,  # TODO: update this if CUDA is not on your machine\n",
      "    data_parallel: False\n",
      "}\n",
      "random_seed: 1447\n",
      "output_dir: ./tutorial/training_outputs/model #UPDATE\n",
      "create_subdirectory: False\n",
      "load_test_set: False\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "with open('all_neuron_tutorial.yml') as f:\n",
    "    contents = f.read()\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Above shows the .yml file used to train the minimal working example model in this workflow. \n",
    "\n",
    "To use your own data you must update the:\n",
    "\n",
    "1. n_targets (Number of Features you are training on):\n",
    "\n",
    "`n_targets: 7`\n",
    "\n",
    "2. Feature list file (a list of all the distinct features you are training on):\n",
    "\n",
    "`features: !obj:selene_sdk.utils.load_features_list {\n",
    "        input_path: /mnt/mfs/statgen/Anmol/training_files/tutorial/tutorial_features.txt #UPDATE\n",
    "    }`\n",
    "    \n",
    "3. Target Path File (A combined bed file for all of your features):\n",
    "\n",
    "`target_path: /mnt/mfs/statgen/Anmol/training_files/tutorial/tutorial.bed.gz, #UPDATE`\n",
    "\n",
    "4. Max_Steps (Maximum number of Training Steps), n_validation_samples (Number of Validation Samples), n_test_samples (Number of Testing Samples):\n",
    "\n",
    "`train_model: !obj:selene_sdk.TrainModel {\n",
    "    batch_size: 64,\n",
    "    max_steps: 501,  # update this value for longer training\n",
    "    report_stats_every_n_steps: 250,\n",
    "    n_validation_samples: 6000,\n",
    "    n_test_samples: 22000,\n",
    "    cpu_n_threads: 32,\n",
    "    use_cuda: False,  # TODO: update this if CUDA is not on your machine\n",
    "    data_parallel: False\n",
    "}`\n",
    "\n",
    "Generally you want to train until the validation and training loss are not decreasing anymore. For the full 2032 features example, I have found that this occurs at around 250,000 training steps which is what I set the max_steps parameter to for that case. Depending on how many features you are using, this could be more or less. You will want to change how frequently the model statistics (ROC,AUC, etc) are outputted especially if you are training on a large number of steps. For 250,000 steps, I would recommend setting the `report_stats_every_n_steps` parameter to 10,000 so you can assess how the model is training frequently enough but you are not wasting too much time calculating the ROC and AUC too frequently. For Validation and Testing Samples, I used 40,000 and 500,000 respectively for 2032 features. Again this will depend on the amount of features you have so adjust this number to be more or less depending on the number of features you are training the model on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Getting Predictions from Trained Model:\n",
    "\n",
    "**Workflow Command to get predictions from trained model for Minimal Example:** `sos run LDSC_DeepSea_Minimal_Example.ipynb make_annot --feature_list /mnt/mfs/statgen/Anmol/training_files/tutorial/tutorial_features.txt --model /mnt/mfs/statgen/Anmol/training_files/tutorial/tutorial/training_outputs/model --output_tsv /mnt/mfs/statgen/Anmol/training_files/tutorial/testing --num_features 7 --vcf /mnt/mfs/statgen/Anmol/training_files/tutorial/tutorial_1000G_chr_`\n",
    "\n",
    "**Explanations of Parameters so that you can change to run with your own data:**\n",
    "\n",
    "1. feature_list: Path to list of distinct features, same as one used in .yml file\n",
    "\n",
    "2. model: Path to location of trained model folder, is the output_dir parameter in the .yml training file\n",
    "\n",
    "3. output_tsv: Path to directory where you want to output the predictions to \n",
    "\n",
    "4. num_features: Number of Features you trained the model on\n",
    "\n",
    "5. vcf: Path to location of reference genome vcf files you want to use for predictions, program will append the chromosome number and .vcf to the end of file name automatically to loop over all the chromosomes so format file name in command as 1000G_chr_ and leave the chr numbers and .vcf out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Formatting Resulting Predictions to Annotation File for LDSC: \n",
    "\n",
    "**Workflow Command to format Prediction files to LDSC Annotation Files:** `sos run LDSC_DeepSea_Minimal_Example.ipynb format_annot --tsv /mnt/mfs/statgen/Anmol/training_files/tutorial/testing --annot_files /mnt/mfs/statgen/Anmol/training_files/tutorial/annot_files\n",
    "\n",
    "**Explanations of Parameters so that you can change to run with your own data:**\n",
    "\n",
    "1. tsv: Path to where prediction files (.tsv files) are located\n",
    "\n",
    "2. annot_files: Path to location where you want the annotation files to be outputted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python 3 (ipykernel)"
   },
   "source": [
    "## Train Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "\n",
    "[train_model]\n",
    "\n",
    "bash: container='/mnt/mfs/statgen/Anmol/deepsea_latest.sif'\n",
    "\n",
    "    python3.7 /mnt/mfs/statgen/Anmol/training_files/tutorial/run_neuron_full_tutorial.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Make Full Annotation File Based on Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Get Predictions for Features based on Trained Model\n",
    "\n",
    "\n",
    "[make_annot]\n",
    "\n",
    "#path to feature list file\n",
    "parameter: feature_list = str\n",
    "#path to trained model location\n",
    "parameter: model = str\n",
    "#path to output directory\n",
    "parameter: output_tsv = str\n",
    "#number of features\n",
    "parameter: num_features = int\n",
    "#VCF files for Reference Genome [Give in this format: tutorial_1000G_chr_, as program will loop over chromsomes and add vcf extension automatically]\n",
    "parameter: vcf = str()\n",
    "\n",
    "\n",
    "python3: container='/mnt/mfs/statgen/Anmol/deepsea_latest.sif',expand = \"${ }\"\n",
    "\n",
    "    from selene_sdk.utils import load_path\n",
    "    from selene_sdk.utils import parse_configs_and_run\n",
    "    from selene_sdk.predict import AnalyzeSequences\n",
    "    from selene_sdk.sequences import Genome\n",
    "    from selene_sdk.utils import load_features_list\n",
    "    from selene_sdk.utils import NonStrandSpecific\n",
    "    from selene_sdk.utils import DeeperDeepSEA\n",
    "    import glob\n",
    "    import os\n",
    "    distinct_features = load_features_list('${feature_list}')\n",
    "\n",
    "    model_predict = AnalyzeSequences(\n",
    "    NonStrandSpecific(DeeperDeepSEA(1000,${num_features})),\n",
    "    '${model}'+\"/best_model.pth.tar\",\n",
    "    sequence_length=1000,\n",
    "    features=distinct_features,\n",
    "    reference_sequence=Genome(\"/mnt/mfs/statgen/Anmol/training_files/male.hg19.fasta\"),\n",
    "    use_cuda=False # update this to False if you do not have CUDA on your machine.\n",
    "    )\n",
    "\n",
    "    for i in range(1,23):\n",
    "        model_predict.variant_effect_prediction(\n",
    "        ${vcf}+str(i)+\".vcf\",\n",
    "        save_data=[\"abs_diffs\"],  # only want to save the absolute diff score data\n",
    "        output_dir='${output_tsv}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Format Annotation File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Separate Annotation Files by Chromosome\n",
    "\n",
    "\n",
    "[format_annot]\n",
    "\n",
    "#path to tsv files [Give in this format: tutorial_1000G_chr_, as program will loop over chromsomes and add tsv extension automatically]\n",
    "parameter: tsv = path()\n",
    "#path to output file directory\n",
    "parameter: annot_files = path()\n",
    "\n",
    "R: expand = \"${ }\", container=\"/mnt/mfs/statgen/Anmol/r-packages.sif\"\n",
    "    library(data.table)\n",
    "    library(tidyverse)\n",
    "    data = fread(paste0(\"${tsv}\",22,\"_abs_diffs.tsv\"))\n",
    "    features = colnames(data)[9:ncol(data)]\n",
    "    features = data.frame(features)\n",
    "    features$encoding = paste0(\"feat_\",seq(1,nrow(features)))\n",
    "    fwrite(features,paste0(\"${annot_files}\",\"/feature_encoding.txt\"),quote=F,sep=\"\\t\",row.names=F,col.names=T)\n",
    "    for (i in seq(1,22)){\n",
    "    data = fread(paste0(\"${tsv}\",i,\"_abs_diffs.tsv\"))\n",
    "    data_2 = select(data,-seq(4,8))\n",
    "    base = data.frame(base=rep(1,nrow(data_2)))\n",
    "    fwrite(base,paste0(\"${annot_files}\",\"/base_chr_\",i,\".annot.gz\"),quote=F,sep=\"\\t\",row.names=F,col.names=T)\n",
    "    for (j in seq(4,ncol(data_2))){\n",
    "    data_3 = select(data_2,c(1,2,3,j))\n",
    "    colnames(data_3) = c(\"CHR\",\"BP\",\"SNP\",paste0(\"feat_\",j))\n",
    "    data_3 = setorder(data_3,BP)\n",
    "    data_3 = select(data_3,-c(\"CHR\",\"BP\",\"SNP\"))\n",
    "    fwrite(data_3,paste0(\"${annot_files}\",\"/feat_\",j,\"_chr_\",i,\".annot.gz\"),quote=F,sep=\"\\t\",row.names=F,col.names=T)\n",
    "    }\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0
   },
   "version": "0.22.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
