{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# WGS GVCF samples joint calling, filtering and annotation\n",
    "\n",
    "Implementing a GATK + ANNOVAR workflow in [SoS](https://github.com/vatlab/SOS), written by Isabelle Schrauwen with software containers built by Gao Wang. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <table class=\"revision_table\">\n",
       "        <tr>\n",
       "        <th>Revision</th>\n",
       "        <th>Author</th>\n",
       "        <th>Date</th>\n",
       "        <th>Message</th>\n",
       "        <tr>\n",
       "        <tr><td><a target=\"_blank\" href=\"git@github.com:cumc/bioworkflows/blob/afe343c7a569b6bc3c33146f102a27c056f87614/gatk_joint_calling.ipynb\"><span class=\"revision_id\">afe343c<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2020-08-20</td>\n",
       "<td>Add variant calling pipeline</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%revisions -s -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This SoS workflow notebook contains three workflows:\n",
    "\n",
    "- `gatk_call`\n",
    "- `gatk_filter`\n",
    "- `annovar`\n",
    "\n",
    "All workflow steps are numerically ordered to reflect the execution logic. This is the most straightforward SoS workflow style, the \"process-oriented\" style. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input data\n",
    "\n",
    "Samples in `GVCF` format, already indexed:\n",
    "\n",
    "```\n",
    "*.gvcf.gz\n",
    "*.gvcf.gz.tbi\n",
    "```\n",
    "\n",
    "To input the list of samples to the workflow, please include all sample file names you would like to analyze, in a text file. For example:\n",
    "\n",
    "```\n",
    "GH.AR.SAD.P1.001.0_X3547_S42_1180478_GVCF.hard-filtered.gvcf.gz\n",
    "GH.AR.SAD.P1.003.0_92455_S43_1189700_GVCF.hard-filtered.gvcf.gz\n",
    "GH.AR.SAD.P1.004.0_92456_S44_1189701_GVCF.hard-filtered.gvcf.gz\n",
    "GH.AR.SAD.P1.005.0_92457_S20_1189702_GVCF.hard-filtered.gvcf.gz\n",
    "...\n",
    "```\n",
    "\n",
    "and save it as, eg, `20200820_sample_manifest.txt`. This text file will be the input file to the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Reference data preparation\n",
    "\n",
    "Human genome reference files are needed for `GATK` joint calling; `ANNOVAR` database references are needed for `ANNOVAR` annotations.\n",
    "\n",
    "- `GATK` reference files include:\n",
    "\n",
    "```\n",
    "*.fa\n",
    "*.fa.fai\n",
    "*.dict\n",
    "```\n",
    "\n",
    "- `ANNOVAR` reference files ship with `ANNOVAR` software, under a folder called `humandb`.\n",
    "\n",
    "This workflow assumes that the required files already have been downloaded. It does not provide steps to download them automatically. However the pipeline will indeed check the availability of the reference files and quit on error if they are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Run the workflow\n",
    "\n",
    "The workflow is currently designed to run on a Linux cluster (via `singularity`) although it can also be executed on a Mac computer\n",
    "(via `docker`). In brief, after installing [SoS](https://github.com/vatlab/SOS) (also see section \"Software Configuration\" below), \n",
    "you can choose to run different workflows modules.\n",
    "\n",
    "For example to run the variant calling workflow,\n",
    "\n",
    "```\n",
    "sos run gatk_joint_calling.ipynb call \\\n",
    "    --vcf-prefix /path/to/some_vcf_file_prefix \\\n",
    "    --samples /path/to/list/of/sample_gvcf.txt \\\n",
    "    --samples-dir /path/to/sample_gvcf \\\n",
    "    --ref-genome /path/to/reference_genome.fa \\\n",
    "    ...\n",
    "```\n",
    "\n",
    "to run variant filtering, \n",
    "\n",
    "\n",
    "```\n",
    "sos run gatk_joint_calling.ipynb filter \\\n",
    "    --vcf-prefix /path/to/some_vcf_file_prefix \\\n",
    "    ...\n",
    "```\n",
    "\n",
    "to run annotation,\n",
    "\n",
    "```\n",
    "sos run gatk_joint_calling.ipynb annovar \\\n",
    "    --vcf-prefix /path/to/some_vcf_file_prefix \\\n",
    "    ...\n",
    "```\n",
    "\n",
    "You can put all these 3 commands to one bash file and execute that, so you run all steps one after another.\n",
    "\n",
    "Note that `...` are additional options that fall into two categories:\n",
    "\n",
    "1. Options needed to run the bioinformatics steps\n",
    "2. Options needed for SoS to run on different platforms\n",
    "\n",
    "To view all options,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run gatk_joint_calling.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  call\n",
      "  filter\n",
      "  annovar\n",
      "\n",
      "Global Workflow Options:\n",
      "  --vcf-prefix VAL (as path, required)\n",
      "                        Combined VCF file prefix, including path to the output\n",
      "                        but without vcf.gz extension, eg\n",
      "                        \"/path/to/output_filename\".\n",
      "  --build hg19\n",
      "                        Human genome build\n",
      "\n",
      "Sections\n",
      "  call_1:               Combine gvcfs\n",
      "    Workflow Options:\n",
      "      --samples VAL (as path, required)\n",
      "                        A file listing out all sample GVCF you would like to\n",
      "                        analyze. Each line is one sample GVCF name.\n",
      "      --samples-dir VAL (as path, required)\n",
      "                        Directory where sample GVCF files locate.\n",
      "      --ref-genome refs/Homo_sapiens.GRCh37.75.dna_sm.primary_assembly.fa (as path)\n",
      "                        Path to reference genome file\n",
      "  call_2:               Joint calling\n",
      "  filter_1:             Split into SNP and INDEL for separate PASS filters\n",
      "  filter_2:             PASS or filter for indels and SNPs (Note | not\n",
      "                        recommended for filters) Ignore MQRankSum warnings <-\n",
      "                        can only be calculated for het sites (not homs)\n",
      "    Workflow Options:\n",
      "      --snp-filters QD < 2.0, QD2 QUAL < 30.0, QUAL30 SOR > 3.0, SOR3 FS > 60.0, FS60 MQ < 40.0, MQ40 MQRankSum < -12.5, MQRankSum-12.5 ReadPosRankSum < -8.0, ReadPosRankSum-8 (as list)\n",
      "      --indel-filters QD < 2.0, QD2 QUAL < 30.0, QUAL30 FS > 200.0, FS200 ReadPosRankSum < -20.0, ReadPosRankSum-20 (as list)\n",
      "  filter_3:             Merge back SNP and INDEL\n",
      "  filter_4:             remove non-PASS variants if wanted\n",
      "  annovar_1:            Annotate\n",
      "    Workflow Options:\n",
      "      --humandb humandb (as path)\n",
      "                        humandb path for ANNOVAR\n",
      "      --x-ref humandb/mart_export_2019_LOFtools3.txt (as path)\n",
      "                        add xreffile to option without -exonicsplicing\n",
      "                        mart_export_2019_LOFtools3.txt #xreffile latest option\n",
      "                        -> Phenotype description,HGNC symbol,MIM morbid descript\n",
      "                        ion,CGD_CONDITION,CGD_inh,CGD_man,CGD_comm,LOF_tools\n",
      "      --protocol refGene refGeneWithVer knownGene ensGene wgEncodeBroadHmmGm12878HMM wgEncodeBroadHmmHmecHMM wgEncodeBroadHmmHepg2HMM wgEncodeBroadHmmH1hescHMM wgEncodeRegDnaseClusteredV3 wgEncodeRegTfbsClusteredV3 genomicSuperDups wgRna targetScanS phastConsElements46way tfbsConsSites gwasCatalog gnomad211_genome gnomad211_exome popfreq_max_20150413 gme kaviar_20150923 abraom avsnp150 dbnsfp35a dbscsnv11 regsnpintron cadd13gt20 clinvar_20200316 mcap13 gene4denovo201907 (as list)\n",
      "                        Annovar protocol\n",
      "      --operation g g g gx r r r r r r r r r r r r f f f f f f f f f f f f f f (as list)\n",
      "                        Annovar operation\n",
      "      --arg \"-splicing 12 -exonicsplicing\" \"-splicing 30\" \"-splicing 12 -exonicsplicing\" \"-splicing 12\"                           (as list)\n",
      "                        Annovar args\n",
      "  annovar_2:            Filter out common variants (from 3 databases) in\n",
      "                        splice_exonic with annovar\n"
     ]
    }
   ],
   "source": [
    "sos run gatk_joint_calling.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "Please read these options carefully before you start running the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Minimal working example\n",
    "\n",
    "Joint calling:\n",
    "\n",
    "```\n",
    "sos run gatk_joint_calling.ipynb call \\\n",
    "    --vcf-prefix output/minimal_example \\\n",
    "    --samples data/samples.txt --samples-dir data \\\n",
    "    --ref-genome refs/Homo_sapiens.GRCh37.75.dna_sm.primary_assembly.fa\n",
    "```\n",
    "\n",
    "Filtering:\n",
    "\n",
    "```\n",
    "sos run gatk_joint_calling.ipynb filter \\\n",
    "    --vcf-prefix output/minimal_example\n",
    "```\n",
    "\n",
    "Annotating:\n",
    "\n",
    "```\n",
    "sos run gatk_joint_calling.ipynb annovar \\\n",
    "    --vcf-prefix output/minimal_example.snp_indel.filter.PASS \\\n",
    "    --humandb refs/humandb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Software configuration\n",
    "\n",
    "Instructions on SoS and docker installation can be found on [our CSG wiki](http://statgen.us/lab-wiki/orientation/jupyter-setup.html). \n",
    "The instructions works for both Mac and Linux, unless otherwise specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Global parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Combined VCF file prefix, including path to the output but without vcf.gz extension, \n",
    "# eg \"/path/to/output_filename\".\n",
    "parameter: vcf_prefix = path\n",
    "# Human genome build\n",
    "parameter: build = 'hg19'\n",
    "# Software container option\n",
    "parameter: container_option = 'gaow/gatk4-annovar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Joint variant calling from GVCF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Combine GVCF files\n",
    "[call_1]\n",
    "# A file listing out all sample GVCF you would like to analyze. \n",
    "# Each line is one sample GVCF name.\n",
    "parameter: samples = path\n",
    "# Directory where sample GVCF files locate.\n",
    "parameter: samples_dir = path()\n",
    "# Path to reference genome file\n",
    "parameter: ref_genome = path('refs/Homo_sapiens.GRCh37.75.dna_sm.primary_assembly.fa')\n",
    "#\n",
    "fail_if(not samples.is_file(), msg = 'Need valid sample name list file input via ``--samples`` option!')\n",
    "import os\n",
    "sample_files = [f'{samples_dir}/{os.path.basename(x.strip())}' for x in open(samples).readlines()]\n",
    "for x in sample_files:\n",
    "    fail_if(not path(x).is_file(), msg = f'Cannot find file ``{x}``. Please use ``--samples-dir`` option to specify the directory for sample files.')\n",
    "    fail_if(not x.endswith('gvcf.gz'), msg = f'Input file ``{x}`` does not have ``.gvcf.gz`` extension.')\n",
    "    if not path(x + '.tbi').is_file():\n",
    "        print(f\"Building index for {x} ...\")\n",
    "        status = get_output(f\"tabix -p vcf {x}\")\n",
    "fail_if(len(sample_files) == 0, msg = 'Need at least one input sample file!')\n",
    "fail_if(not ref_genome.is_file(), msg = f'Cannot find reference genome ``{ref_genome}``. Please use ``--ref-genome`` option to specify it.')\n",
    "fail_if(not path(f\"{ref_genome:a}.fai\").is_file(), msg = f'Cannot find reference genome index file ``{ref_genome}.fai``. Please make sure it exists.')\n",
    "fail_if(not path(f\"{ref_genome:an}.dict\").is_file(), msg = f'Cannot find reference genome dict file ``{ref_genome:n}.dict``. Please make sure it exists.')\n",
    "\n",
    "depends: system_resource(mem = '12G'), ref_genome\n",
    "input: sample_files\n",
    "output: f'{vcf_prefix:a}.combined.vcf.gz'\n",
    "\n",
    "bash: container=container_option, volumes=[f'{ref_genome:ad}:{ref_genome:ad}'], expand=\"${ }\", stderr=f'{_output:n}.err', stdout=f'{_output:n}.out'\n",
    "    gatk --java-options \"-Xmx12g\" CombineGVCFs \\\n",
    "        -R ${ref_genome} \\\n",
    "        ${' '.join(['--variant %s' % x for x in _input])} \\\n",
    "        -O ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Joint calling\n",
    "[call_2]\n",
    "# Path to reference genome file\n",
    "parameter: ref_genome = path\n",
    "output: f'{vcf_prefix:a}.vcf.gz'\n",
    "\n",
    "\n",
    "bash: container=container_option, volumes=[f'{ref_genome:ad}:{ref_genome:ad}'], expand=\"${ }\", stderr=f'{_output:nn}.err', stdout=f'{_output:nn}.out'\n",
    "    gatk --java-options \"-Xmx12g\" GenotypeGVCFs \\\n",
    "        -R ${ref_genome} \\\n",
    "        -V ${_input} \\\n",
    "        -O ${_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Variant filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Split into SNP and INDEL for separate PASS filters\n",
    "[filter_1]\n",
    "variant_type = ['SNP', 'INDEL']\n",
    "input: f'{vcf_prefix:a}.vcf.gz', for_each='variant_type', concurrent = True\n",
    "output: f'{vcf_prefix:a}.{_variant_type.lower()}.vcf.gz'\n",
    "\n",
    "\n",
    "bash: container=container_option, expand=\"${ }\", stderr=f'{_output:nn}.err', stdout=f'{_output:nn}.out'\n",
    "    gatk --java-options '-Xmx12g' SelectVariants \\\n",
    "        -V ${_input} \\\n",
    "        -select-type ${_variant_type} \\\n",
    "        -O ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# PASS or filter for indels and SNPs (Note | not recommended for filters)\n",
    "# Ignore MQRankSum warnings <- can only be calculated for het sites (not homs)\n",
    "[filter_2]\n",
    "parameter: snp_filters = ['QD < 2.0, QD2', 'QUAL < 30.0, QUAL30', 'SOR > 3.0, SOR3', 'FS > 60.0, FS60', 'MQ < 40.0, MQ40', 'MQRankSum < -12.5, MQRankSum-12.5', 'ReadPosRankSum < -8.0, ReadPosRankSum-8']\n",
    "parameter: indel_filters = [\"QD < 2.0, QD2\", \"QUAL < 30.0, QUAL30\", \"FS > 200.0, FS200\", \"ReadPosRankSum < -20.0, ReadPosRankSum-20\"]\n",
    "input: paired_with = dict(filter_option=[snp_filters, indel_filters])\n",
    "output: f'{_input:nn}.filter.vcf.gz'\n",
    "\n",
    "\n",
    "bash: container=container_option, expand=\"${ }\", stderr=f'{_output:nn}.err', stdout=f'{_output:nn}.out'\n",
    "    gatk --java-options '-Xmx12g' VariantFiltration \\\n",
    "        -V ${_input} \\\n",
    "        ${\" \".join(['-filter \"%s\" --filter-name \"%s\"' % tuple([y.strip() for y in x.split(',')]) for x in _input.filter_option])} \\\n",
    "        -O ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Merge back SNP and INDEL\n",
    "[filter_3]\n",
    "input: group_by = 'all'\n",
    "output: f'{vcf_prefix:a}.snp_indel.filter.vcf.gz'\n",
    "\n",
    "\n",
    "bash: container=container_option, expand=\"${ }\", stderr=f'{_output:nn}.err', stdout=f'{_output:nn}.out'\n",
    "    gatk --java-options '-Xmx12g' MergeVcfs \\\n",
    "     -I ${_input[0]} -I ${_input[1]} -O ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# remove non-PASS variants if wanted\n",
    "[filter_4]\n",
    "output: f'{vcf_prefix:a}.snp_indel.filter.PASS.vcf.gz'\n",
    "\n",
    "\n",
    "bash: container=container_option, expand=\"${ }\", stderr=f'{_output:nn}.err', stdout=f'{_output:nn}.out'\n",
    "    gatk --java-options '-Xmx12g' SelectVariants \\\n",
    "        -V ${_input} -O ${_output} \\\n",
    "        --exclude-filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Annotate \n",
    "[annovar_1]\n",
    "# humandb path for ANNOVAR\n",
    "parameter: humandb = path\n",
    "#add xreffile to option without -exonicsplicing\n",
    "#mart_export_2019_LOFtools3.txt #xreffile latest option -> Phenotype description,HGNC symbol,MIM morbid description,CGD_CONDITION,CGD_inh,CGD_man,CGD_comm,LOF_tools\n",
    "parameter: x_ref = path(f\"{humandb}/mart_export_2019_LOFtools3.txt\")\n",
    "# Annovar protocol\n",
    "parameter: protocol = ['refGene', 'refGeneWithVer', 'knownGene', 'ensGene', 'wgEncodeBroadHmmGm12878HMM', 'wgEncodeBroadHmmHmecHMM', 'wgEncodeBroadHmmHepg2HMM', 'wgEncodeBroadHmmH1hescHMM', 'wgEncodeRegDnaseClusteredV3', 'wgEncodeRegTfbsClusteredV3', 'genomicSuperDups', 'wgRna', 'targetScanS', 'phastConsElements46way', 'tfbsConsSites', 'gwasCatalog', 'gnomad211_genome', 'gnomad211_exome', 'popfreq_max_20150413', 'gme', 'kaviar_20150923', 'abraom', 'avsnp150', 'dbnsfp35a', 'dbscsnv11', 'regsnpintron', 'cadd13gt20', 'clinvar_20200316', 'mcap13', 'gene4denovo201907']\n",
    "# Annovar operation\n",
    "parameter: operation = ['g', 'g', 'g', 'gx', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f']\n",
    "# Annovar args\n",
    "parameter: arg = ['\"-splicing 12 -exonicsplicing\"', '\"-splicing 30\"', '\"-splicing 12 -exonicsplicing\"', '\"-splicing 12\"', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
    "input: f'{vcf_prefix:a}.vcf.gz'\n",
    "output: f'{_input:nn}.annovar.hg19_multianno_path.txt', f'{_input:nn}.annovar.hg19_multianno_splice_exon.txt'\n",
    "\n",
    "\n",
    "bash: container=container_option, volumes=[f'{humandb:a}:{humandb:a}'], expand=\"${ }\", stderr=f'{_output[0]:n}.err', stdout=f'{_output[0]:n}.out'\n",
    "    #do not add -intronhgvs as option -> writes cDNA variants as HGVS but creates issues (+2 splice site reported only)\n",
    "    #-nastring . can only be . for VCF files\n",
    "    #regsnpintron might cause shifted lines (be carefull using)\n",
    "    set -e\n",
    "    table_annovar.pl \\\n",
    "        ${_input} \\\n",
    "        ${humandb} \\\n",
    "        -buildver ${build} \\\n",
    "        -out ${_output[0]:nn} \\\n",
    "        -remove \\\n",
    "        -polish \\\n",
    "        -otherinfo \\\n",
    "        -nastring . \\\n",
    "        -protocol ${\",\".join(protocol)} \\\n",
    "        -operation ${\",\".join(operation)} \\\n",
    "        -arg ${\",\".join(arg)} \\\n",
    "        -vcfinput -xreffile ${x_ref}\n",
    "    #keep pathogenic <- keep unfiltered;\n",
    "    awk 'FNR == 1 {print} /pathogenic|Pathogenic/{print}' ${_output[0]:nn}.hg19_multianno.txt  > ${_output[0]}\n",
    "    #keep splice_exonic\n",
    "    awk 'FNR == 1 {print} /splic|exonic/{print}' ${_output[0]:nn}.hg19_multianno.txt  > ${_output[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Filter out common variants (from 3 databases) in splice_exonic with annovar \n",
    "[annovar_2]\n",
    "# humandb path for ANNOVAR\n",
    "parameter: humandb = path(\"humandb/\")\n",
    "input: group_by = 1, concurrent = True\n",
    "output: f'{_input:nn}.gnomad211_exome_genome_popfreq_max'\n",
    "\n",
    "bash: container=container_option, volumes=[f'{humandb:a}:{humandb:a}'], expand=\"${ }\", stderr=f'{_output:n}.err', stdout=f'{_output:n}.out'\n",
    "    set -e\n",
    "    annotate_variation.pl -filter -dbtype gnomad211_exome \\\n",
    "        -build ${build} \\\n",
    "        -score_threshold 0.005 \\\n",
    "        ${_input} \\\n",
    "        ${humandb} \\\n",
    "        -out ${_input:nn}.gnomad211_exome\n",
    "    \n",
    "    annotate_variation.pl -filter -dbtype gnomad211_genome \\\n",
    "        -build ${build} \\\n",
    "        -score_threshold 0.005 \\\n",
    "        ${_input:nn}.gnomad211_exome \\\n",
    "        ${humandb} \\\n",
    "        -out ${_input:nn}.gnomad211_exome_genome\n",
    "\n",
    "    annotate_variation.pl -filter -dbtype popfreq_max_20150413 \\\n",
    "        -build ${build} \\\n",
    "        -score_threshold 0.005 \\\n",
    "        ${_input:nn}.gnomad211_exome_genome \\\n",
    "        ${humandb} \\\n",
    "        -out ${_input:nn}.gnomad211_exome_genome_popfreq_max\n",
    "    #rm ${_input:nn}.*_dropped"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "default_kernel": "SoS",
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   },
   "version": "0.21.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
